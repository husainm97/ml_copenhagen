{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lyw35ahWTsk"
      },
      "source": [
        "# Shakespearean Transformer (nano-GPT) with BPE Tokenization\n",
        "\n",
        "This hands-on exercise touches on building and training a miniature version of a Generative Pre-trained Transformer (GPT) model.\n",
        "\n",
        "This notebook is heavily inspired by and simplifies Andrej Karpathy's [nanoGPT repository](https://github.com/karpathy/nanoGPT). The goal is to work on the basics needed to build a character-level language model on the works of Shakespeare.\n",
        "\n",
        "**Why is this exciting?**\n",
        "The Transformer architecture, is the foundation of modern Large Language Models (LLMs). While our model will be much smaller, the fundamental building blocks (like self-attention, positional embeddings, and decoder blocks) are conceptually very similar to those in models like GPT-3, which powered the original ChatGPT. Hopefully this helps gain deeper intuition for how these models work.\n",
        "\n",
        "**Project outline:**\n",
        "1.  **Data Preparation:** Load Shakespeare's text and create a character-level tokenizer.\n",
        "2.  **Model Definition:** Implement the GPT architecture from scratch, including self-attention and transformer blocks.\n",
        "3.  **Training:** Write a simple training loop to teach our model to predict the next character in a sequence.\n",
        "4.  **Generation:** Use our trained model to generate new, Shakespeare-like text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdGxvM88P90L",
        "outputId": "3dd159e2-7f11-48b1-a1e7-b6abfa1041a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import pickle # For saving/loading meta later if needed, though not strictly for tokenizer now\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "BATCH_SIZE = 64\n",
        "BLOCK_SIZE = 256  # Context length\n",
        "MAX_ITERS = 5000\n",
        "EVAL_INTERVAL = 250\n",
        "LEARNING_RATE = 3e-4 # Adjusted from 1e-3, often 3e-4 is a good starting point\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "N_EMBD = 384\n",
        "N_HEAD = 6\n",
        "N_LAYER = 10\n",
        "DROPOUT = 0.2\n",
        "# AdamW optimizer betas\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.95\n",
        "# Early stopping\n",
        "EARLY_STOPPING_PATIENCE = 5 # Number of evaluation intervals to wait\n",
        "EVAL_ITERS_FOR_LOSS = 100 # Number of batches to average for loss estimation\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(1337)\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGA8_ajuWrJ0"
      },
      "source": [
        "## 1. Data Loading and Tokenization\n",
        "\n",
        "The first step in any machine learning project is to prepare the data. For our character-level language model, this involves:\n",
        "1.  **Downloading the Data:** We'll use the \"Tiny Shakespeare\" dataset, which contains a collection of Shakespeare's works.\n",
        "2.  **Creating a Vocabulary:** We'll identify all unique characters present in the text. This set of unique characters will form our vocabulary.\n",
        "3.  **Building a Tokenizer:** We'll create two simple functions:\n",
        "    *   `encode`: Converts a string of characters into a list of corresponding integer IDs (tokens).\n",
        "    *   `decode`: Converts a list of integer IDs back into a string of characters.\n",
        "4.  **Splitting the Data:** We'll divide the dataset into a training set (for teaching the model) and a validation set (for evaluating its performance on unseen data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-aKr2lTRHv4",
        "outputId": "91b8fe75-6a43-4b90-a6aa-62e375a054f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.txt already exists.\n",
            "Length of dataset in characters (after dropping lines): 5,447,700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BPE merges: 100%|██████████| 1000/1000 [55:14<00:00,  3.31s/merge]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE training took 3314.42 seconds\n",
            "Encoding took 4.78 seconds\n",
            "BPE Vocabulary size: 1,023\n",
            "Train data has 4,902,930 tokens\n",
            "Validation data has 544,770 tokens\n",
            "\n",
            "--- Demonstration ---\n",
            "Original Text: Friends, Romans, countrymen, lend me your ears;\n",
            "Encoded Tokens: [31, 73, 64, 60, 69, 59, 74, 8, 1, 43, 70, 68, 56, 69, 74, 8, 1, 58, 70, 76, 69, 75, 73, 80, 68, 60, 69, 8, 1, 67, 60, 69, 59, 1, 68, 60, 1, 80, 70, 76, 73, 1, 60, 56, 73, 74, 22]\n",
            "Decoded Text: Friends, Romans, countrymen, lend me your ears;\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Data Loading and Tokenization ---\n",
        "input_file_path = 'input.txt'\n",
        "data_url = 'https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    print(f\"Downloading {input_file_path}...\")\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(f\"{input_file_path} already exists.\")\n",
        "\n",
        "# Read the data and discard the first 250 lines\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    data = \"\".join(lines[250:]) # Discard the first 250 lines\n",
        "\n",
        "print(f\"Length of dataset in characters (after dropping lines): {len(data):,}\")\n",
        "\n",
        "def get_words(text_data):\n",
        "    tokens = re.findall(r'\\s+|\\w+|[^\\w\\s]', text_data, re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "# --- Simple Sub-word Tokenization (BPE) ---\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SimpleBPETokenizer:\n",
        "    def __init__(self, num_merges, min_frequency=2):\n",
        "        self.num_merges = num_merges\n",
        "        self.min_frequency = min_frequency\n",
        "        self.merges = {}\n",
        "        self.vocab = {}\n",
        "        self.itos = {}\n",
        "        self.stoi = {}\n",
        "\n",
        "    def train(self, corpus):\n",
        "        # Ensure corpus is a string\n",
        "        if isinstance(corpus, list):\n",
        "            corpus = \" \".join(corpus)\n",
        "\n",
        "        # Initialize character-level vocab including spaces\n",
        "        initial_tokens = set(corpus)\n",
        "        self.vocab = {token: idx for idx, token in enumerate(sorted(initial_tokens))}\n",
        "        self.itos = {idx: token for token, idx in self.vocab.items()}\n",
        "        self.stoi = dict(self.vocab)\n",
        "\n",
        "        for _ in tqdm(range(self.num_merges), desc=\"BPE merges\", unit=\"merge\"):\n",
        "            pair_counts = defaultdict(int)\n",
        "            for word in corpus.split():  # split by space\n",
        "                tokens = self._tokenize_word(word, self.merges)\n",
        "                for j in range(len(tokens) - 1):\n",
        "                    pair = (tokens[j], tokens[j + 1])\n",
        "                    pair_counts[pair] += 1\n",
        "\n",
        "            # Filter by min_frequency\n",
        "            filtered_pairs = {pair: count for pair, count in pair_counts.items() if count >= self.min_frequency}\n",
        "            if not filtered_pairs:\n",
        "                break\n",
        "\n",
        "            most_frequent_pair = max(filtered_pairs, key=filtered_pairs.get)\n",
        "            new_token = \"\".join(most_frequent_pair)\n",
        "            self.merges[most_frequent_pair] = new_token\n",
        "            new_id = len(self.vocab)\n",
        "            self.vocab[new_token] = new_id\n",
        "            self.itos[new_id] = new_token\n",
        "            self.stoi = dict(self.vocab)\n",
        "\n",
        "    def _tokenize_word(self, word, merges):\n",
        "        tokens = list(word)\n",
        "        while True:\n",
        "            best_pair = None\n",
        "            for pair in zip(tokens[:-1], tokens[1:]):\n",
        "                if pair in merges:\n",
        "                    best_pair = pair\n",
        "                    break\n",
        "            if best_pair is None:\n",
        "                break\n",
        "\n",
        "            new_tokens = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
        "                    new_tokens.append(merges[best_pair])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = []\n",
        "        # Treat every character including spaces\n",
        "        for char in text:\n",
        "            subwords = self._tokenize_word(char, self.merges)\n",
        "            tokens.extend([self.stoi[subword] for subword in subwords if subword in self.stoi])\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        # Join tokens directly, spaces are preserved\n",
        "        return \"\".join([self.itos.get(tok_id, \"\") for tok_id in token_ids])\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. Tokenize Data using BPE ---\n",
        "training_words = get_words(data)\n",
        "bpe_tokenizer = SimpleBPETokenizer(num_merges=1000)\n",
        "\n",
        "start_train = time.time()\n",
        "bpe_tokenizer.train(training_words)\n",
        "print(f\"BPE training took {time.time() - start_train:.2f} seconds\")\n",
        "\n",
        "# Create train and validation splits\n",
        "n = len(data)\n",
        "train_text = data[:int(n * 0.9)]\n",
        "val_text = data[int(n * 0.9):]\n",
        "\n",
        "start_encode = time.time()\n",
        "train_data = torch.tensor(bpe_tokenizer.encode(train_text), dtype=torch.long)\n",
        "val_data = torch.tensor(bpe_tokenizer.encode(val_text), dtype=torch.long)\n",
        "print(f\"Encoding took {time.time() - start_encode:.2f} seconds\")\n",
        "\n",
        "print(f\"BPE Vocabulary size: {len(bpe_tokenizer.vocab):,}\")\n",
        "print(f\"Train data has {len(train_data):,} tokens\")\n",
        "print(f\"Validation data has {len(val_data):,} tokens\")\n",
        "\n",
        "# --- Demonstration of BPE ---\n",
        "sample_text = \"Friends, Romans, countrymen, lend me your ears;\"\n",
        "encoded_tokens = bpe_tokenizer.encode(sample_text)\n",
        "decoded_text = bpe_tokenizer.decode(encoded_tokens)\n",
        "\n",
        "print(\"\\n--- Demonstration ---\")\n",
        "print(f\"Original Text: {sample_text}\")\n",
        "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
        "print(f\"Decoded Text: {decoded_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cco935wtRLmE",
        "outputId": "95177680-cf79-4045-a845-a3c081ab75b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved to bpe_tokenizer.pkl.\n",
            "Tokenizer loaded from bpe_tokenizer.pkl.\n",
            "Original text: 'Hello, World! This is a test.'\n",
            "Encoded text: [33, 60, 67, 67, 70, 8, 1, 48, 70, 73, 67, 59, 2, 1, 45, 63, 64, 74, 1, 64, 74, 1, 56, 1, 75, 60, 74, 75, 10]\n",
            "Decoded text: 'Hello, World! This is a test.'\n",
            "Tokenizer test passed!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# --- Save the trained tokenizer object ---\n",
        "with open(\"bpe_tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(bpe_tokenizer, f)\n",
        "print(\"Tokenizer saved to bpe_tokenizer.pkl.\")\n",
        "\n",
        "# --- Load tokenizer object directly ---\n",
        "with open(\"bpe_tokenizer.pkl\", \"rb\") as f:\n",
        "    loaded_tokenizer = pickle.load(f)\n",
        "print(\"Tokenizer loaded from bpe_tokenizer.pkl.\")\n",
        "\n",
        "# --- Tokenizer Experimentation ---\n",
        "sample_text = \"Hello, World! This is a test.\"\n",
        "print(f\"Original text: '{sample_text}'\")\n",
        "\n",
        "encoded_sample = loaded_tokenizer.encode(sample_text)\n",
        "print(f\"Encoded text: {encoded_sample}\")\n",
        "\n",
        "decoded_sample = loaded_tokenizer.decode(encoded_sample)\n",
        "print(f\"Decoded text: '{decoded_sample}'\")\n",
        "\n",
        "assert sample_text == decoded_sample, \"Encoding/Decoding mismatch!\"\n",
        "print(\"Tokenizer test passed!\")\n",
        "\n",
        "vocab_size = len(bpe_tokenizer.vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ihert8KRQvf",
        "outputId": "4b182e3e-344d-4f65-9e34-8d811bf772e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([64, 256])\n",
            "Target batch shape: torch.Size([64, 256])\n",
            "First sequence in input batch (first 30 tokens): think I'll weep.\n",
            "     No, I'll not weep.\n",
            "     I have full cause of weeping, but this heart\n",
            "     Shal\n",
            "First sequence in target batch (first 30 tokens): hink I'll weep.\n",
            "     No, I'll not weep.\n",
            "     I have full cause of weeping, but this heart\n",
            "     Shall\n"
          ]
        }
      ],
      "source": [
        "# --- Data Loader ---\n",
        "def get_batch(split):\n",
        "    # Selects the appropriate dataset (train or val)\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    # Generates random starting indices for batch_size sequences\n",
        "    ix = torch.randint(len(data_source) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "    # Extracts input sequences (x)\n",
        "    x = torch.stack([data_source[i:i+BLOCK_SIZE] for i in ix])\n",
        "    # Extracts target sequences (y), which are shifted by one character\n",
        "    y = torch.stack([data_source[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    # Move data to the specified device (CPU or GPU)\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    return x, y\n",
        "\n",
        "# Test get_batch\n",
        "xb, yb = get_batch('train')\n",
        "print(\"Input batch shape:\", xb.shape)\n",
        "print(\"Target batch shape:\", yb.shape)\n",
        "print(\"First sequence in input batch (first 30 tokens):\", loaded_tokenizer.decode(xb[0][:100].tolist()))\n",
        "print(\"First sequence in target batch (first 30 tokens):\", loaded_tokenizer.decode(yb[0][:100].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW9lylKWW1ao"
      },
      "source": [
        "## 2. Model Definition: The Transformer Architecture\n",
        "\n",
        "Now for the exciting part: building our GPT model! We'll implement the core components of the Transformer architecture, specifically the \"decoder-only\" variant used in GPT models.\n",
        "\n",
        "Key components we will define:\n",
        "*   **`GPTConfig`**: A dataclass to hold all our model hyperparameters (like vocabulary size, embedding dimension, number of layers, etc.).\n",
        "*   **`LayerNorm`**: A normalization layer crucial for stabilizing training in deep networks.\n",
        "*   **`CausalSelfAttention`**: The heart of the Transformer! This module allows each token in a sequence to \"attend\" to previous tokens (but not future ones, hence \"causal\") to understand context. We'll implement the scaled dot-product attention mechanism.\n",
        "*   **`MLP` (Multi-Layer Perceptron)**: A simple feed-forward network applied after the attention mechanism in each Transformer block.\n",
        "*   **`Block`**: A single Transformer block, which typically consists of a self-attention layer followed by an MLP, with residual connections and layer normalization.\n",
        "*   **`GPT`**: The main model class that stacks multiple `Block`s, includes token and positional embeddings, and a final linear layer to predict the next token.\n",
        "\n",
        "We'll also explore the model's output *before* any training to understand its initial (random) state and the expected initial loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5nODNxK-RUdE"
      },
      "outputs": [],
      "source": [
        "# --- 2. Model Definition ---\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = BLOCK_SIZE\n",
        "    vocab_size: int = vocab_size # Will be set by loaded data\n",
        "    n_layer: int = N_LAYER\n",
        "    n_head: int = N_HEAD\n",
        "    n_embd: int = N_EMBD\n",
        "    dropout: float = DROPOUT\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. \"\"\"\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        # We use register_buffer for parameters that should be part of the model's state\n",
        "        # but are not trained by the optimizer (e.g., a fixed mask).\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                    .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # Manual implementation of attention\n",
        "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # Apply causal mask\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs side by side\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU() # Using GELU activation\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))  # Attention with residual connection\n",
        "        x = x + self.mlp(self.ln_2(x))   # MLP with residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Zy_tgqySRgax"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),      # Token embeddings\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),     # Positional embeddings\n",
        "            drop = nn.Dropout(config.dropout),                        # Dropout layer\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # Transformer blocks\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),        # Final layer norm\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Language model head\n",
        "\n",
        "        # Weight tying: token embeddings and final linear layer share weights\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"Number of trainable parameters: {num_params/1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size() # Batch size, sequence length\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # Shape (t)\n",
        "\n",
        "        # Forward the GPT model\n",
        "        tok_emb = self.transformer.wte(idx) # Token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # Position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # If we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x) # (b, t, vocab_size)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        else:\n",
        "            # Inference-time optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # Note: using list [-1] to preserve the time dim -> (b, 1, vocab_size)\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        for _ in range(max_new_tokens):\n",
        "            # If the sequence context is growing too long, crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # Forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond) # Loss is None during generation\n",
        "            # Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf') # Mask non-top-k logits\n",
        "            # Apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        self.train() # Set model back to training mode if used elsewhere\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WVoHathRq0E",
        "outputId": "ab5bc587-1341-4460-d7ab-5012b5f6a008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 18.24M\n",
            "--- Before Training ---\n",
            "Logits shape: torch.Size([64, 256, 1023])\n",
            "Initial loss: 6.855808734893799\n",
            "Expected initial loss (approx -ln(1/vocab_size)): 6.9305\n",
            "Logits for the first token prediction in the first sequence (first 10 values): tensor([ 0.1112,  0.3710,  0.6394, -0.4221,  0.1925,  0.5556,  0.4003,  0.1992,\n",
            "        -0.0651,  0.4096], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# --- Model Initialization and Initial Exploration ---\n",
        "\n",
        "# Create GPTConfig instance\n",
        "model_config = GPTConfig(vocab_size=vocab_size, block_size=BLOCK_SIZE,\n",
        "                         n_layer=N_LAYER, n_head=N_HEAD, n_embd=N_EMBD, dropout=DROPOUT)\n",
        "\n",
        "# Instantiate the model\n",
        "model = GPT(model_config)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Get a batch of data\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print(\"--- Before Training ---\")\n",
        "# Pass the batch through the UNTRAINED model\n",
        "logits, loss = model(xb, yb)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape) # Should be (BATCH_SIZE, BLOCK_SIZE, vocab_size)\n",
        "print(\"Initial loss:\", loss.item())\n",
        "\n",
        "# For a randomly initialized model, the loss should be roughly -ln(1/vocab_size)\n",
        "# This is because the model initially assigns roughly equal probability to each token in the vocabulary.\n",
        "# The cross-entropy loss for a uniform distribution over V classes is -sum( (1/V) * log(1/V) ) = -V * (1/V) * log(1/V) = -log(1/V) = log(V)\n",
        "expected_loss = -math.log(1.0 / vocab_size)\n",
        "print(f\"Expected initial loss (approx -ln(1/vocab_size)): {expected_loss:.4f}\")\n",
        "# The actual initial loss will vary due to specific weight initialization but should be in this ballpark.\n",
        "\n",
        "# Let's look at the logits for the first token prediction in the first sequence\n",
        "first_token_logits = logits[0, 0, :]\n",
        "print(\"Logits for the first token prediction in the first sequence (first 10 values):\", first_token_logits[:10])\n",
        "# These are raw, unnormalized scores for each possible next character.\n",
        "# After softmax, these would turn into probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZNucyqirRtaZ"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation Function ---\n",
        "@torch.no_grad() # Decorator to disable gradient calculations during evaluation\n",
        "def estimate_loss(model_to_eval):\n",
        "    out = {}\n",
        "    model_to_eval.eval() # Set the model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(EVAL_ITERS_FOR_LOSS) # Array to store losses for averaging\n",
        "        for k in range(EVAL_ITERS_FOR_LOSS):\n",
        "            X, Y = get_batch(split)\n",
        "            _, current_loss = model_to_eval(X, Y)\n",
        "            losses[k] = current_loss.item()\n",
        "        out[split] = losses.mean() # Average loss over EVAL_ITERS_FOR_LOSS batches\n",
        "    model_to_eval.train() # Set the model back to training mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Chb-oYW_2W"
      },
      "source": [
        "## 3. Training the Model\n",
        "\n",
        "With the data prepared and model defined, we're ready to train! The training process involves:\n",
        "1.  **Loss Estimation:** We'll define a helper function (`estimate_loss`) to calculate the model's performance (cross-entropy loss) on both the training and validation sets without updating its weights. This helps us monitor learning and detect overfitting.\n",
        "2.  **Optimizer:** We'll use the AdamW optimizer, a common choice for training Transformers.\n",
        "3.  **Training Loop:**\n",
        "    *   Repeatedly sample batches of data.\n",
        "    *   Perform a **forward pass**: Feed the input batch to the model to get predictions (logits) and calculate the loss.\n",
        "    *   Perform a **backward pass**: Calculate gradients of the loss with respect to the model's parameters.\n",
        "    *   **Update parameters**: Adjust the model's weights using the optimizer to minimize the loss.\n",
        "4.  **Evaluation & Checkpointing:** Periodically, we'll evaluate the model on the validation set. If the validation loss improves, we'll save a \"checkpoint\" of the model's weights.\n",
        "5.  **Early Stopping:** If the validation loss stops improving for a certain number of evaluations, we'll stop training to prevent overfitting.\n",
        "\n",
        "We'll use `tqdm` to display a progress bar during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g-m4EsXR64X",
        "outputId": "1536e27e-68d6-4be5-8710-41967a526d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/10000 [00:11<31:12:07, 11.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step     0: Train loss 2.5254, Val loss 2.5419\n",
            "  -> New best validation loss: 2.5419. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   3%|▎         | 251/10000 [01:03<9:31:09,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step   250: Train loss 2.0729, Val loss 2.0845\n",
            "  -> New best validation loss: 2.0845. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   5%|▌         | 501/10000 [01:56<9:16:37,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step   500: Train loss 1.7602, Val loss 1.7825\n",
            "  -> New best validation loss: 1.7825. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   8%|▊         | 751/10000 [02:48<9:01:50,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step   750: Train loss 1.5435, Val loss 1.6032\n",
            "  -> New best validation loss: 1.6032. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  10%|█         | 1001/10000 [03:41<8:47:35,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  1000: Train loss 1.4195, Val loss 1.4932\n",
            "  -> New best validation loss: 1.4932. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 1251/10000 [04:34<8:33:06,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  1250: Train loss 1.3378, Val loss 1.4427\n",
            "  -> New best validation loss: 1.4427. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  15%|█▌        | 1501/10000 [05:26<8:18:18,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  1500: Train loss 1.2841, Val loss 1.3997\n",
            "  -> New best validation loss: 1.3997. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  18%|█▊        | 1751/10000 [06:19<8:03:29,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  1750: Train loss 1.2381, Val loss 1.3746\n",
            "  -> New best validation loss: 1.3746. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 2001/10000 [07:12<7:49:04,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  2000: Train loss 1.2117, Val loss 1.3572\n",
            "  -> New best validation loss: 1.3572. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  23%|██▎       | 2251/10000 [08:04<7:34:10,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  2250: Train loss 1.1799, Val loss 1.3322\n",
            "  -> New best validation loss: 1.3322. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  25%|██▌       | 2501/10000 [08:57<7:19:17,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  2500: Train loss 1.1570, Val loss 1.3140\n",
            "  -> New best validation loss: 1.3140. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  28%|██▊       | 2751/10000 [09:49<7:04:57,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  2750: Train loss 1.1408, Val loss 1.3114\n",
            "  -> New best validation loss: 1.3114. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  30%|███       | 3001/10000 [10:42<6:50:08,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  3000: Train loss 1.1237, Val loss 1.2839\n",
            "  -> New best validation loss: 1.2839. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 3251/10000 [11:35<6:35:55,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  3250: Train loss 1.1071, Val loss 1.2754\n",
            "  -> New best validation loss: 1.2754. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  35%|███▌      | 3502/10000 [12:27<4:28:47,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  3500: Train loss 1.0929, Val loss 1.2756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  38%|███▊      | 3751/10000 [13:20<6:06:18,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  3750: Train loss 1.0847, Val loss 1.2670\n",
            "  -> New best validation loss: 1.2670. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 4001/10000 [14:12<5:51:14,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  4000: Train loss 1.0739, Val loss 1.2575\n",
            "  -> New best validation loss: 1.2575. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  43%|████▎     | 4251/10000 [15:05<5:36:48,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  4250: Train loss 1.0648, Val loss 1.2536\n",
            "  -> New best validation loss: 1.2536. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  45%|████▌     | 4501/10000 [15:58<5:22:33,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  4500: Train loss 1.0522, Val loss 1.2428\n",
            "  -> New best validation loss: 1.2428. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  48%|████▊     | 4751/10000 [16:50<5:07:42,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  4750: Train loss 1.0432, Val loss 1.2349\n",
            "  -> New best validation loss: 1.2349. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  50%|█████     | 5001/10000 [17:43<4:52:58,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  5000: Train loss 1.0405, Val loss 1.2315\n",
            "  -> New best validation loss: 1.2315. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 5252/10000 [18:35<3:16:12,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  5250: Train loss 1.0239, Val loss 1.2401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  55%|█████▌    | 5501/10000 [19:28<4:23:54,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  5500: Train loss 1.0124, Val loss 1.2247\n",
            "  -> New best validation loss: 1.2247. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  58%|█████▊    | 5751/10000 [20:21<4:09:02,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  5750: Train loss 1.0118, Val loss 1.2217\n",
            "  -> New best validation loss: 1.2217. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 6002/10000 [21:13<2:45:09,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  6000: Train loss 1.0045, Val loss 1.2243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  63%|██████▎   | 6251/10000 [22:06<3:39:45,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  6250: Train loss 0.9942, Val loss 1.2167\n",
            "  -> New best validation loss: 1.2167. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  65%|██████▌   | 6501/10000 [22:58<3:24:52,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  6500: Train loss 0.9855, Val loss 1.2076\n",
            "  -> New best validation loss: 1.2076. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  68%|██████▊   | 6752/10000 [23:51<2:14:07,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  6750: Train loss 0.9834, Val loss 1.2102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  70%|███████   | 7002/10000 [24:43<2:03:49,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  7000: Train loss 0.9788, Val loss 1.2190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 7252/10000 [25:36<1:53:30,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  7250: Train loss 0.9652, Val loss 1.2116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  75%|███████▌  | 7502/10000 [26:28<1:43:10,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  7500: Train loss 0.9617, Val loss 1.2102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  78%|███████▊  | 7750/10000 [27:21<07:56,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step  7750: Train loss 0.9542, Val loss 1.2097\n",
            "Early stopping triggered at iteration 7750 after 5 evaluation intervals without improvement.\n",
            "--- Training Finished ---\n",
            "Total training time: 27.35 minutes\n",
            "Best validation loss achieved: 1.2076\n",
            "Loading best model weights for generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Training Loop ---\n",
        "print(f\"Starting training on {DEVICE}...\")\n",
        "\n",
        "MAX_ITERS = 10000\n",
        "# Optimizer (AdamW is a common choice for transformers)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "iter_history = []\n",
        "\n",
        "# For timing\n",
        "overall_start_time = time.time()\n",
        "\n",
        "# Wrap the main loop with tqdm for a progress bar\n",
        "for iter_num in tqdm(range(MAX_ITERS), desc=\"Training Progress\"):\n",
        "    # iter_start_time = time.time() # tqdm provides timing per iteration\n",
        "\n",
        "    # Every EVAL_INTERVAL, estimate loss on train and val sets\n",
        "    if iter_num % EVAL_INTERVAL == 0 or iter_num == MAX_ITERS - 1:\n",
        "        losses = estimate_loss(model) # This already sets model.eval() and model.train()\n",
        "        train_loss_history.append(losses['train'].item())\n",
        "        val_loss_history.append(losses['val'].item())\n",
        "        iter_history.append(iter_num)\n",
        "\n",
        "        # tqdm will show iteration/s, so manual timing print might be redundant\n",
        "        # elapsed_iter_time = time.time() - iter_start_time\n",
        "        # print(f\"Step {iter_num}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}, Iter time: {elapsed_iter_time:.2f}s\")\n",
        "\n",
        "        # Log to tqdm's postfix\n",
        "        tqdm.write(f\"Step {iter_num:5d}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model checkpoint to the Colab instance's local storage\n",
        "            # This file will be lost if the Colab runtime is disconnected or reset unless moved.\n",
        "            torch.save(model.state_dict(), 'best_shakespeare_model.pth')\n",
        "            tqdm.write(f\"  -> New best validation loss: {best_val_loss:.4f}. Model saved to best_shakespeare_model.pth\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "            tqdm.write(f\"Early stopping triggered at iteration {iter_num} after {EARLY_STOPPING_PATIENCE} evaluation intervals without improvement.\")\n",
        "            break # Exit the training loop\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Forward pass: evaluate loss\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    optimizer.zero_grad(set_to_none=True) # Zero out gradients from previous iteration\n",
        "    loss.backward()                       # Compute gradients\n",
        "    optimizer.step()                      # Update model parameters\n",
        "\n",
        "overall_end_time = time.time()\n",
        "print(f\"--- Training Finished ---\")\n",
        "print(f\"Total training time: {(overall_end_time - overall_start_time)/60:.2f} minutes\")\n",
        "print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
        "\n",
        "# Load the best model weights after training is complete (or early stopped)\n",
        "# This ensures the 'model' variable holds the best performing weights for generation.\n",
        "if os.path.exists('best_shakespeare_model.pth'):\n",
        "    print(\"Loading best model weights for generation...\")\n",
        "    model.load_state_dict(torch.load('best_shakespeare_model.pth', map_location=DEVICE))\n",
        "else:\n",
        "    print(\"Warning: No 'best_shakespeare_model.pth' found. Using current model state (likely the last trained state) for generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWjsFttMXR5e"
      },
      "source": [
        "## 4. Generating Text with the Trained Model\n",
        "\n",
        "After training (or loading our best checkpoint), our model should have learned the patterns and style of Shakespearean English. Now, we can use it as a generative model:\n",
        "1.  **Provide a Prompt:** We'll give the model a starting sequence of characters (a \"prompt\").\n",
        "2.  **Predict Next Character:** The model will predict the probability distribution for the next character.\n",
        "3.  **Sample:** We'll sample a character from this distribution (we can use `temperature` to control randomness and `top_k` to limit choices).\n",
        "4.  **Append and Repeat:** Append the sampled character to our sequence and feed the new, longer sequence back into the model to generate the next character. We repeat this process to generate new text.\n",
        "\n",
        "Let's see if our model can write some \"new\" Shakespeare!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "883M7aH8R-AF",
        "outputId": "45eb6d7c-d6b5-4b12-cdbc-d52b3230a97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Shakespeare-like text ---\n",
            "Starting prompt: 'The king is dead!'\n",
            "\n",
            "--- Generated Text ---\n",
            "The king is dead! What breaker, stay!\n",
            "    They are provented the King in the one,\n",
            "    Such many such a palace that makes your princes,\n",
            "    And so am I do make you between them.\n",
            "    But merry was incens'd to say well to-morrow.\n",
            "    But good to me, I shall not come to my master.\n",
            "    Then, I am not convenient. Do so, fair father;\n",
            "    Father, father, for I do obtain the sight,\n",
            "    And with thee had a sport of guiltless knowledge,\n",
            "    And many bands of meat thee made thee hears,\n",
            "    And stretch thee from the mercy and loyalty.\n",
            "    And yet, gentle lord, I am almost fond;\n",
            "    Yet, so I think I would not make thee end.\n",
            "    Henry, lords, a lovely roof and depart;\n",
            "    Where stands thy brother is of thy heart.\n",
            "    There is thy life, and lives no slow.\n",
            "  KING RICHARD. Upon the troops of thy heavy hearts.\n",
            "                                                       What plays  \n",
            "    Where is it?\n",
            "  WARWICK. Sir Richard Bolingbroke.\n",
            "  KING RICHARD. Was it a dagger then?\n",
            "  KING RICHARD. Ay, but not to know what thou dost tak\n"
          ]
        }
      ],
      "source": [
        "# --- 4. Generate Text ---\n",
        "print(f\"\\n--- Generating Shakespeare-like text ---\")\n",
        "\n",
        "# You can change the starting prompt\n",
        "# start_string = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\n\"\n",
        "# start_string = \"To be, or not to be, that is the question:\\n\"\n",
        "start_string = \"The king is dead!\"\n",
        "\n",
        "print(f\"Starting prompt: '{start_string.strip()}'\")\n",
        "\n",
        "start_ids = loaded_tokenizer.encode(start_string)\n",
        "# Unsqueeze to add batch dimension: (seq_len) -> (1, seq_len)\n",
        "x_input = torch.tensor(start_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "# Generate text\n",
        "model.eval() # Set model to evaluation mode for generation\n",
        "with torch.no_grad(): # No need to track gradients during generation\n",
        "    generated_ids = model.generate(x_input,\n",
        "                                   max_new_tokens=1000,\n",
        "                                   temperature=0.8, # Controls randomness: lower is less random, higher is more random\n",
        "                                   top_k=20)       # Considers only the top_k most likely tokens at each step\n",
        "\n",
        "generated_text = loaded_tokenizer.decode(generated_ids[0].tolist()) # Decode the first (and only) batch item\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fhFw5opUSBFA",
        "outputId": "1d128214-f018-4358-db45-73c03ba962f4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if train_loss_history and val_loss_history:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(iter_history, train_loss_history, label='Training Loss')\n",
        "    plt.plot(iter_history, val_loss_history, label='Validation Loss')\n",
        "    plt.xlabel(f'Iteration (x{1})') # Since iter_history stores actual iteration numbers\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Over Iterations')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No loss history to plot. Did training run?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xrUrvc3XZeg"
      },
      "source": [
        "## Further Exploration!\n",
        "\n",
        "**1. Hyperparameter Tuning:**\n",
        "    *   Experiment with `LEARNING_RATE`, `N_LAYER`, `N_HEAD`, `N_EMBD`, `DROPOUT`. How do these changes affect training speed, final validation loss, and the quality of generated text?\n",
        "    *   What happens if you significantly increase `MAX_ITERS` (and adjust `EARLY_STOPPING_PATIENCE` if needed)? Can you get a lower validation loss? (Be mindful of Colab time limits).\n",
        "    *   Try different `temperature` and `top_k` values during generation. How does it change the output?\n",
        "\n",
        "**7. More Exercises:**\n",
        "        *   How does weight initialization affect training?\n",
        "        *   What if you remove positional embeddings?\n",
        "        *   Can you train on a different small text dataset?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1TLO6gYp5x-"
      },
      "source": [
        "## Bonus: Saving and Loading Your Trained Model\n",
        "\n",
        "Here's how we can permanently save your model and load it back:\n",
        "\n",
        "#### Downloading the Model to Your Local Computer\n",
        "\n",
        "Once training is complete and `best_shakespeare_model.pth` has been saved by the script:\n",
        "\n",
        "1.  **Locate the File:** In the Colab interface, click on the \"Files\" icon (looks like a folder) in the left sidebar. You should see `best_shakespeare_model.pth` listed.\n",
        "2.  **Download:** Right-click on `best_shakespeare_model.pth` and select \"Download\". The file will be saved to your computer's default downloads folder.\n",
        "\n",
        "To load this model back into a *new* Colab session (or on your local machine if you have PyTorch set up):\n",
        "1.  Upload the `.pth` file back to the Colab session (using the \"Upload to session storage\" button in the Files tab).\n",
        "2.  Then, in your code, you can load it (make sure your model architecture is defined first):\n",
        "    ```python\n",
        "    # # --- Code to define the GPT model and GPTConfig must be run first ---\n",
        "    # model_config = GPTConfig(vocab_size=vocab_size, ...) # Use the same config as training\n",
        "    # model = GPT(model_config)\n",
        "    # model.to(DEVICE)\n",
        "    #\n",
        "    # # Load the state dictionary\n",
        "    # model_path = 'best_shakespeare_model.pth' # Or whatever you named it when uploading\n",
        "    # if os.path.exists(model_path):\n",
        "    #     model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    #     model.eval() # Set to evaluation mode\n",
        "    #     print(f\"Model loaded from {model_path}\")\n",
        "    # else:\n",
        "    #     print(f\"Model file not found at {model_path}\")\n",
        "    ```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v26ISDuSEoo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}