{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1289cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import pickle # For saving/loading meta later if needed, though not strictly for tokenizer now\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 256  # Context length\n",
    "MAX_ITERS = 5000\n",
    "EVAL_INTERVAL = 250\n",
    "LEARNING_RATE = 3e-4 # Adjusted from 1e-3, often 3e-4 is a good starting point\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_EMBD = 384\n",
    "N_HEAD = 6\n",
    "N_LAYER = 10\n",
    "DROPOUT = 0.2\n",
    "# AdamW optimizer betas\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.95\n",
    "# Early stopping\n",
    "EARLY_STOPPING_PATIENCE = 5 # Number of evaluation intervals to wait\n",
    "EVAL_ITERS_FOR_LOSS = 100 # Number of batches to average for loss estimation\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(1337)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges, min_frequency=2):\n",
    "        self.num_merges = num_merges\n",
    "        self.min_frequency = min_frequency\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "        self.itos = {}\n",
    "        self.stoi = {}\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Ensure corpus is a string\n",
    "        if isinstance(corpus, list):\n",
    "            corpus = \" \".join(corpus)\n",
    "\n",
    "        # Initialize character-level vocab including spaces\n",
    "        initial_tokens = set(corpus)\n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted(initial_tokens))}\n",
    "        self.itos = {idx: token for token, idx in self.vocab.items()}\n",
    "        self.stoi = dict(self.vocab)\n",
    "\n",
    "        for _ in tqdm(range(self.num_merges), desc=\"BPE merges\", unit=\"merge\"):\n",
    "            pair_counts = defaultdict(int)\n",
    "            for word in corpus.split():  # split by space\n",
    "                tokens = self._tokenize_word(word, self.merges)\n",
    "                for j in range(len(tokens) - 1):\n",
    "                    pair = (tokens[j], tokens[j + 1])\n",
    "                    pair_counts[pair] += 1\n",
    "\n",
    "            # Filter by min_frequency\n",
    "            filtered_pairs = {pair: count for pair, count in pair_counts.items() if count >= self.min_frequency}\n",
    "            if not filtered_pairs:\n",
    "                break\n",
    "\n",
    "            most_frequent_pair = max(filtered_pairs, key=filtered_pairs.get)\n",
    "            new_token = \"\".join(most_frequent_pair)\n",
    "            self.merges[most_frequent_pair] = new_token\n",
    "            new_id = len(self.vocab)\n",
    "            self.vocab[new_token] = new_id\n",
    "            self.itos[new_id] = new_token\n",
    "            self.stoi = dict(self.vocab)\n",
    "\n",
    "    def _tokenize_word(self, word, merges):\n",
    "        tokens = list(word)\n",
    "        while True:\n",
    "            best_pair = None\n",
    "            for pair in zip(tokens[:-1], tokens[1:]):\n",
    "                if pair in merges:\n",
    "                    best_pair = pair\n",
    "                    break\n",
    "            if best_pair is None:\n",
    "                break\n",
    "\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                    new_tokens.append(merges[best_pair])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        # Treat every character including spaces\n",
    "        for char in text:\n",
    "            subwords = self._tokenize_word(char, self.merges)\n",
    "            tokens.extend([self.stoi[subword] for subword in subwords if subword in self.stoi])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Join tokens directly, spaces are preserved\n",
    "        return \"\".join([self.itos.get(tok_id, \"\") for tok_id in token_ids])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    vocab_size: int = vocab_size # Will be set by loaded data\n",
    "    n_layer: int = N_LAYER\n",
    "    n_head: int = N_HEAD\n",
    "    n_embd: int = N_EMBD\n",
    "    dropout: float = DROPOUT\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        # We use register_buffer for parameters that should be part of the model's state\n",
    "        # but are not trained by the optimizer (e.g., a fixed mask).\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # Manual implementation of attention\n",
    "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # Apply causal mask\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs side by side\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU() # Using GELU activation\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))  # Attention with residual connection\n",
    "        x = x + self.mlp(self.ln_2(x))   # MLP with residual connection\n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),      # Token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),     # Positional embeddings\n",
    "            drop = nn.Dropout(config.dropout),                        # Dropout layer\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # Transformer blocks\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),        # Final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Language model head\n",
    "\n",
    "        # Weight tying: token embeddings and final linear layer share weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Number of trainable parameters: {num_params/1e6:.2f}M\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size() # Batch size, sequence length\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # Shape (t)\n",
    "\n",
    "        # Forward the GPT model\n",
    "        tok_emb = self.transformer.wte(idx) # Token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # Position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x) # (b, t, vocab_size)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            # Inference-time optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # Note: using list [-1] to preserve the time dim -> (b, 1, vocab_size)\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        \"\"\"\n",
    "        self.eval() # Set model to evaluation mode\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If the sequence context is growing too long, crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # Forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond) # Loss is None during generation\n",
    "            # Pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # Optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf') # Mask non-top-k logits\n",
    "            # Apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        self.train() # Set model back to training mode if used elsewhere\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16f595e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 18.24M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(1023, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-9): 10 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=1023, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = GPTConfig(vocab_size=vocab_size, block_size=BLOCK_SIZE,\n",
    "                         n_layer=N_LAYER, n_head=N_HEAD, n_embd=N_EMBD, dropout=DROPOUT)\n",
    "\n",
    "# Instantiate the model\n",
    "model = GPT(model_config)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "923dcb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best_shakespeare_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_216370/220410139.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# Load the state dictionary\n",
    "model_path = 'best_shakespeare_model.pth' # Or whatever you named it when uploading\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval() # Set to evaluation mode\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "else:\n",
    "    print(f\"Model file not found at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd91e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ef0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from bpe_tokenizer.pkl.\n"
     ]
    }
   ],
   "source": [
    "# --- Load tokenizer object directly ---\n",
    "with open(\"bpe_tokenizer.pkl\", \"rb\") as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "print(\"Tokenizer loaded from bpe_tokenizer.pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a2c2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Shakespeare-like text ---\n",
      "Starting prompt: 'A pound of flesh, I demand!'\n",
      "\n",
      "--- Generated Text ---\n",
      "A pound of flesh, I demand!\n",
      "    O honour! and let me have our noble heart,\n",
      "    And you shall have mov'd the mountains of all,\n",
      "    And perceive this dreams of mercy death,\n",
      "    But I will come forth to put your wealth,\n",
      "    And with a speech of fortune's dearest mouth,\n",
      "    Who may be false doubtful and perforce\n",
      "    The crown poor or and child of wilful looks.\n",
      "    Come, away, but never take thee mercy.\n",
      "                                                          Exeunt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scene IV.\n",
      "A hall in Venice. Another part of the city\n",
      "\n",
      "The other Duke of York and his hands of Gloucester.\n",
      "\n",
      "Enter the Lord Constable and Lord Hastings\n",
      "\n",
      "  Con. See, with milk that hangs on the forest and straight.\n",
      "    Let them go away; or else the bond of wrath,\n",
      "    Let have their belded beard at the town,\n",
      "    And lay their looks and sous cowarded shall look on.\n",
      "    They are stain'd in their tears, the enemy,\n",
      "    And have their power to march their deaths.\n",
      "    And from the hands is but their extreme shall raise  \n",
      "    With reasons is become his prosper\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Generate Text ---\n",
    "print(f\"\\n--- Generating Shakespeare-like text ---\")\n",
    "\n",
    "# You can change the starting prompt\n",
    "# start_string = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\n\"\n",
    "# start_string = \"To be, or not to be, that is the question:\\n\"\n",
    "start_string = \"A pound of flesh, I demand!\"\n",
    "\n",
    "print(f\"Starting prompt: '{start_string.strip()}'\")\n",
    "\n",
    "start_ids = loaded_tokenizer.encode(start_string)\n",
    "# Unsqueeze to add batch dimension: (seq_len) -> (1, seq_len)\n",
    "x_input = torch.tensor(start_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "# Generate text\n",
    "model.eval() # Set model to evaluation mode for generation\n",
    "with torch.no_grad(): # No need to track gradients during generation\n",
    "    generated_ids = model.generate(x_input,\n",
    "                                   max_new_tokens=1000,\n",
    "                                   temperature=0.8, # Controls randomness: lower is less random, higher is more random\n",
    "                                   top_k=20)       # Considers only the top_k most likely tokens at each step\n",
    "\n",
    "generated_text = loaded_tokenizer.decode(generated_ids[0].tolist()) # Decode the first (and only) batch item\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8530e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_copenhagen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
