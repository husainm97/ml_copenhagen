{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59357a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 features: ['pX_maxEcell_energy', 'pX_ecore', 'p_etcone20', 'pX_E_Lr2_MedG', 'p_ptcone40', 'pX_e233', 'pX_etcone20', 'pX_topoetcone20', 'p_pt_track', 'pX_E3x5_Lr0', 'p_ptcone30', 'p_sigmad0']\n",
      "‚è≥ Starting Optuna tuning...\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's rmse: 17716.9\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17715.8\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's rmse: 18130.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmse: 17934.7\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 47575.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's rmse: 17993.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 30874.4\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's rmse: 17729.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's rmse: 18331\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 21771.7\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 51184.3\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 18406.5\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's rmse: 18243\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17789.7\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 27872.5\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's rmse: 17761.2\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's rmse: 17891.3\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17872.2\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 20032.9\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17867.4\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 35338.7\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17830.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 18683.4\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 17676.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 18027\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 23883.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 40556.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17896.9\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 18332.7\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's rmse: 17958.5\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 17918.9\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17791.4\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's rmse: 17849.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17803\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\tvalid_0's rmse: 17949.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17934\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's rmse: 17924.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's rmse: 18069.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's rmse: 17714.8\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's rmse: 18108.2\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's rmse: 17983.1\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 17800.9\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's rmse: 17751.9\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's rmse: 17985.2\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's rmse: 17917.5\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's rmse: 18180.5\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 17960.5\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's rmse: 18010.6\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 19637.4\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's rmse: 18219.5\n",
      "‚úÖ Best hyperparameters: {'learning_rate': 0.13145544908167586, 'num_leaves': 80, 'max_depth': 17, 'min_data_in_leaf': 35, 'feature_fraction': 0.6346101446829857, 'bagging_fraction': 0.7672496173120773, 'bagging_freq': 6, 'lambda_l1': 0.7106859381792285, 'lambda_l2': 0.6616121995691258}\n",
      "‚è±Ô∏è Tuning time: 47.19 seconds\n",
      "‚è≥ Training final model...\n",
      "Training until validation scores don't improve for 7 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's rmse: 17924.1\n",
      "‚úÖ Training time: 0.58 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "df = pd.read_csv('../data/dataset_1/electrons_train.csv')\n",
    "df = df[df[\"p_Truth_isElectron\"] == 1]  # Filter only electrons\n",
    "\n",
    "target = \"p_Truth_Energy\"\n",
    "X = df.drop(columns=[\"p_Truth_isElectron\", \"p_Truth_Energy\"])\n",
    "y = df[target]\n",
    "\n",
    "# 2. Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 3. Feature Selection: Top 12 via RF importance\n",
    "rf_reg = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "feat_importances = pd.Series(rf_reg.feature_importances_, index=X_train.columns)\n",
    "top_12_features = feat_importances.nlargest(12).index.tolist()\n",
    "\n",
    "print(\"Top 12 features:\", top_12_features)\n",
    "\n",
    "'''\n",
    "# 3. Feature Selection: Top 12 via LightGBM importance\n",
    "tmp_lgb = lgb.LGBMRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "tmp_lgb.fit(X_train, y_train)\n",
    "\n",
    "feat_importances = pd.Series(tmp_lgb.feature_importances_, index=X_train.columns)\n",
    "top_12_features = feat_importances.nlargest(12).index.tolist()\n",
    "\n",
    "print(\"Top 12 features (LGBM):\", top_12_features)\n",
    "'''\n",
    "\n",
    "X_train_sel = X_train[top_12_features]\n",
    "X_val_sel = X_val[top_12_features]\n",
    "\n",
    "# Optional scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_sel = scaler.fit_transform(X_train_sel)\n",
    "X_val_sel = scaler.transform(X_val_sel)\n",
    "\n",
    "# 4. Hyperparameter Tuning using Optuna (Bayesian Optimization)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.2, log=True),\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 30, 100),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 16, 24),\n",
    "        'min_data_in_leaf': trial.suggest_int(\"min_data_in_leaf\", 10, 50),\n",
    "        'feature_fraction': trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        'lambda_l1': trial.suggest_float(\"lambda_l1\", 0, 5),\n",
    "        'lambda_l2': trial.suggest_float(\"lambda_l2\", 0, 5)\n",
    "    }\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train_sel, label=y_train)\n",
    "    dval = lgb.Dataset(X_val_sel, label=y_val, reference=dtrain)\n",
    "\n",
    "    gbm = lgb.train(params, train_set=dtrain, valid_sets=[dval], callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=7),\n",
    "        lgb.log_evaluation(0)\n",
    "    ])\n",
    "    preds = gbm.predict(X_val_sel)\n",
    "\n",
    "    rel_mae = np.mean(np.abs((preds - y_val) / y_val))\n",
    "    return rel_mae\n",
    "\n",
    "\n",
    "print(\"‚è≥ Starting Optuna tuning...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=50, timeout=600)\n",
    "optuna_time = time.time() - start_time\n",
    "\n",
    "print(\"‚úÖ Best hyperparameters:\", study.best_params)\n",
    "print(f\"‚è±Ô∏è Tuning time: {optuna_time:.2f} seconds\")\n",
    "\n",
    "# 5. Train final model\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1\n",
    "})\n",
    "\n",
    "train_data = lgb.Dataset(X_train_sel, label=y_train)\n",
    "val_data = lgb.Dataset(X_val_sel, label=y_val, reference=train_data)\n",
    "\n",
    "print(\"‚è≥ Training final model...\")\n",
    "start_train_time = time.time()\n",
    "model = lgb.train(best_params, train_set=train_data, valid_sets=[val_data], callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=7),\n",
    "        lgb.log_evaluation(0)\n",
    "    ])\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\"‚úÖ Training time: {train_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53f271f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation:\n",
      "Relative MAE: 0.3108\n",
      "RMSE: 321271897.0914\n",
      "R¬≤: 0.8970\n",
      "üíæ Saved model size: 0.33 MB\n",
      "üß† Estimated number of split nodes (model params): 3841\n",
      "\n",
      "‚úÖ Best hyperparameters: {'learning_rate': 0.13145544908167586, 'num_leaves': 80, 'max_depth': 17, 'min_data_in_leaf': 35, 'feature_fraction': 0.6346101446829857, 'bagging_fraction': 0.7672496173120773, 'bagging_freq': 6, 'lambda_l1': 0.7106859381792285, 'lambda_l2': 0.6616121995691258, 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1}\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate\n",
    "preds = model.predict(X_val_sel)\n",
    "rel_mae = np.mean(np.abs((preds - y_val) / y_val))\n",
    "rmse = mean_squared_error(y_val, preds)\n",
    "r2 = r2_score(y_val, preds)\n",
    "\n",
    "print(f\"\\nüìä Evaluation:\")\n",
    "print(f\"Relative MAE: {rel_mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "# 7. Save Model\n",
    "model_path = \"regression_lightgbm_model.json\"\n",
    "model.save_model(model_path)\n",
    "\n",
    "# Model size\n",
    "model_size_MB = os.path.getsize(model_path) / (1024**2)\n",
    "print(f\"üíæ Saved model size: {model_size_MB:.2f} MB\")\n",
    "\n",
    "\n",
    "def count_split_nodes(node):\n",
    "    \"\"\"Recursively counts split nodes in a tree structure.\"\"\"\n",
    "    count = 0\n",
    "    if \"split_index\" in node:  # This indicates a split node\n",
    "        count += 1\n",
    "        # Recursively count in children\n",
    "        if \"left_child\" in node:\n",
    "            count += count_split_nodes(node[\"left_child\"])\n",
    "        if \"right_child\" in node:\n",
    "            count += count_split_nodes(node[\"right_child\"])\n",
    "    return count\n",
    "\n",
    "total_split_nodes = 0\n",
    "for tree_info in model.dump_model()[\"tree_info\"]:\n",
    "    total_split_nodes += count_split_nodes(tree_info[\"tree_structure\"])\n",
    "\n",
    "print(f\"üß† Estimated number of split nodes (model params): {total_split_nodes}\")\n",
    "print(f\"\\n‚úÖ Best hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bbf3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions and variable list saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load test data\n",
    "test_df = pd.read_csv('./data/dataset_1/AppML_InitialProject_test_regression.csv')\n",
    "\n",
    "# 2. Select the top 12 features you used during training\n",
    "# Assuming top_12_features variable exists from before or save/load them\n",
    "# Here I just use the variable from your code:\n",
    "\n",
    "# top_12_features = [...]  # Already defined from training\n",
    "\n",
    "# 3. Prepare test data with selected features\n",
    "X_test_sel = test_df[top_12_features]\n",
    "\n",
    "# 4. Apply the same scaler (StandardScaler fitted on training data)\n",
    "X_test_sel_scaled = scaler.transform(X_test_sel)  # scaler fitted on training data\n",
    "\n",
    "# 5. Predict with the trained LightGBM model\n",
    "predictions = model.predict(X_test_sel_scaled)\n",
    "\n",
    "# 6. Save predictions CSV with index and prediction column\n",
    "output_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "output_df.to_csv('Regression_HusainManasawala_LightGBM.csv', index=False, header=False)\n",
    "\n",
    "# 7. Save top 12 feature list CSV\n",
    "features_df = pd.DataFrame({'top_12_features': top_12_features})\n",
    "features_df.to_csv('Regression_HusainManasawala_LightGBM_VariableList.csv', index=False, header=False)\n",
    "\n",
    "# 11. Save variable list\n",
    "with open('Regression_HusainManasawala_LightGBM_VariableList.csv', 'w') as f:\n",
    "    for feature in top_12_features:\n",
    "        f.write(f\"{feature},\\n\")\n",
    "        \n",
    "print(\"‚úÖ Predictions and variable list saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e35810",
   "metadata": {},
   "source": [
    "## Now a Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "991e52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 features: ['pX_maxEcell_energy', 'pX_ecore', 'p_etcone20', 'pX_E_Lr2_MedG', 'p_ptcone40', 'pX_e233', 'pX_etcone20', 'pX_topoetcone20', 'p_pt_track', 'pX_E3x5_Lr0', 'p_ptcone30', 'p_sigmad0']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import time\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- 1. Load and filter data ---\n",
    "df = pd.read_csv('./data/dataset_1/AppML_InitialProject_train.csv')\n",
    "df = df[df[\"p_Truth_isElectron\"] == 1]\n",
    "\n",
    "target = \"p_Truth_Energy\"\n",
    "X = df.drop(columns=[\"p_Truth_isElectron\", \"p_Truth_Energy\"])\n",
    "y = df[target]\n",
    "\n",
    "# --- 2. Train/Val split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 3. Feature selection: Top 12 via RF importance ---\n",
    "rf_reg = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "feat_importances = pd.Series(rf_reg.feature_importances_, index=X_train.columns)\n",
    "top_12_features = feat_importances.nlargest(12).index.tolist()\n",
    "\n",
    "print(\"Top 12 features:\", top_12_features)\n",
    "\n",
    "X_train_sel = X_train[top_12_features]\n",
    "X_val_sel = X_val[top_12_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca08c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "‚è≥ Starting Optuna tuning...\n",
      "‚úÖ Best hyperparameters: {'hidden_dim': 215, 'lr': 0.004446679044470628}\n",
      "‚è±Ô∏è Tuning time: 349.04 seconds\n",
      "‚úÖ Final training time: 83.48 seconds\n",
      "\n",
      "üìä Final Evaluation:\n",
      "Relative MAE: 0.3391\n",
      "RMSE: 18448.2853\n",
      "R¬≤: 0.8909\n",
      "üß† Number of trainable parameters: 142331\n",
      "üíæ Saved model size: 0.55 MB\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_train_sel = scaler.fit_transform(X_train_sel)\n",
    "X_val_sel = scaler.transform(X_val_sel)\n",
    "\n",
    "# Convert to torch tensors (float32)\n",
    "X_train_t = torch.tensor(X_train_sel, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_t = torch.tensor(X_val_sel, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# --- 5. Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 6. Define neural network ---\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- 7. Training function ---\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss = criterion(preds, yb)\n",
    "                val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# --- 8. Evaluation function ---\n",
    "def evaluate(model, X, y, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X.to(device)).cpu().numpy().flatten()\n",
    "    y_true = y.cpu().numpy().flatten()\n",
    "    rel_mae = np.mean(np.abs((preds - y_true) / y_true))\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    r2 = r2_score(y_true, preds)\n",
    "    return rel_mae, rmse, r2, preds\n",
    "\n",
    "# --- 9. Prepare DataLoaders ---\n",
    "def get_dataloader(X, y, batch_size=64):\n",
    "    ds = TensorDataset(X, y)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = get_dataloader(X_train_t, y_train_t)\n",
    "val_loader = get_dataloader(X_val_t, y_val_t, batch_size=256)\n",
    "\n",
    "# --- 10. Optuna objective ---\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 300)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    epochs = 50  # fixed for tuning\n",
    "\n",
    "    model = FeedForwardNN(input_dim=12, hidden_dim=hidden_dim).to(device)\n",
    "    trained_model = train_model(model, train_loader, val_loader, epochs, lr, device)\n",
    "\n",
    "    rel_mae, rmse, r2, _ = evaluate(trained_model, X_val_t, y_val_t, device)\n",
    "    return rel_mae\n",
    "\n",
    "# --- 11. Run Optuna tuning ---\n",
    "print(\"‚è≥ Starting Optuna tuning...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10, timeout=900)  # 10 trials max 15 min timeout\n",
    "optuna_time = time.time() - start_time\n",
    "print(f\"‚úÖ Best hyperparameters: {study.best_params}\")\n",
    "print(f\"‚è±Ô∏è Tuning time: {optuna_time:.2f} seconds\")\n",
    "\n",
    "# --- 12. Train final model ---\n",
    "best_params = study.best_params\n",
    "best_hidden_dim = best_params[\"hidden_dim\"]\n",
    "best_lr = best_params[\"lr\"]\n",
    "epochs_final = 100  # longer training for final\n",
    "\n",
    "model = FeedForwardNN(input_dim=12, hidden_dim=best_hidden_dim).to(device)\n",
    "start_train_time = time.time()\n",
    "model = train_model(model, train_loader, val_loader, epochs_final, best_lr, device)\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\"‚úÖ Final training time: {train_time:.2f} seconds\")\n",
    "\n",
    "# --- 13. Evaluate final model ---\n",
    "rel_mae, rmse, r2, preds = evaluate(model, X_val_t, y_val_t, device)\n",
    "print(\"\\nüìä Final Evaluation:\")\n",
    "print(f\"Relative MAE: {rel_mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "# --- 14. Model params count ---\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üß† Number of trainable parameters: {total_params}\")\n",
    "\n",
    "# --- 15. Save model ---\n",
    "model_path = \"pytorch_nn_regressor.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'features': top_12_features,\n",
    "    'params': best_params\n",
    "}, model_path)\n",
    "\n",
    "model_size_MB = os.path.getsize(model_path) / (1024**2)\n",
    "print(f\"üíæ Saved model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a51a5c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions and variable list saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husainm97/miniconda3/envs/ml_copenhagen_env/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Load test data\n",
    "test_df = pd.read_csv('./data/dataset_1/AppML_InitialProject_test_regression.csv')\n",
    "\n",
    "# 2. Select the top 12 features you used during training\n",
    "X_test_sel = test_df[top_12_features]\n",
    "\n",
    "# 3. Scale using the same scaler fitted on training data\n",
    "X_test_sel_scaled = scaler.transform(X_test_sel)\n",
    "\n",
    "# 4. Convert to torch tensor, move to device (assume device defined)\n",
    "X_test_tensor = torch.tensor(X_test_sel_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# 5. Predict with PyTorch model (eval mode, no grad)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_tensor = model(X_test_tensor).squeeze().cpu()\n",
    "predictions = preds_tensor.numpy()\n",
    "\n",
    "# 6. Save predictions CSV with index and prediction, no header\n",
    "output_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "output_df.to_csv('Regression_HusainManasawala_PyTorchNN.csv', index=False, header=False)\n",
    "\n",
    "# 7. Save top 12 features in the comma + newline style you want\n",
    "with open('Regression_HusainManasawala_PyTorchNN_VariableList.csv', 'w') as f:\n",
    "    for feature in top_12_features:\n",
    "        f.write(f\"{feature},\\n\")\n",
    "\n",
    "print(\"‚úÖ Predictions and variable list saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a8d9e",
   "metadata": {},
   "source": [
    "## Trying XGBoost with RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a86ba1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 features: ['pX_maxEcell_energy', 'pX_ecore', 'p_etcone20', 'pX_E_Lr2_MedG', 'p_ptcone40', 'pX_e233', 'pX_etcone20', 'pX_topoetcone20', 'p_pt_track', 'pX_E3x5_Lr0', 'p_ptcone30', 'p_sigmad0']\n",
      "‚è≥ Starting hyperparameter tuning...\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "‚úÖ Tuning done in 54.37 seconds\n",
      "Best params: {'subsample': np.float64(0.9), 'reg_lambda': 5, 'reg_alpha': 1, 'n_estimators': 300, 'max_depth': 8, 'learning_rate': np.float64(0.059948425031894084), 'colsample_bytree': np.float64(0.7)}\n",
      "‚è≥ Training final model on full data...\n",
      "‚úÖ Training time: 0.54 seconds\n",
      "\n",
      "üìä Evaluation on validation set:\n",
      "Relative MAE: 0.1969\n",
      "RMSE: 92228601.9619\n",
      "R¬≤: 0.9704\n",
      "üíæ Model size: 4.74 MB\n",
      "üíæ Scaler size: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "df = pd.read_csv('./data/dataset_1/AppML_InitialProject_train.csv')\n",
    "df = df[df[\"p_Truth_isElectron\"] == 1]  # only electrons\n",
    "\n",
    "target = \"p_Truth_Energy\"\n",
    "X = df.drop(columns=[\"p_Truth_isElectron\", \"p_Truth_Energy\"])\n",
    "y = df[target]\n",
    "\n",
    "# 2. Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Feature selection: top 12 with RF\n",
    "rf_reg = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "feat_importances = pd.Series(rf_reg.feature_importances_, index=X_train.columns)\n",
    "top_12_features = feat_importances.nlargest(12).index.tolist()\n",
    "print(\"Top 12 features:\", top_12_features)\n",
    "\n",
    "X_train_sel = X_train[top_12_features]\n",
    "X_val_sel = X_val[top_12_features]\n",
    "\n",
    "# 4. Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_sel = scaler.fit_transform(X_train_sel)\n",
    "X_val_sel = scaler.transform(X_val_sel)\n",
    "\n",
    "# 5. Set up XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', eval_metric='rmse', n_jobs=-1, random_state=42)\n",
    "\n",
    "# 6. Randomized Search CV hyperparameter space\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [4, 6, 8, 12, 16],\n",
    "    'learning_rate': np.logspace(-3, -1, 10),\n",
    "    'subsample': np.linspace(0.5, 1.0, 6),\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 6),\n",
    "    'reg_alpha': [0, 0.1, 1, 5],\n",
    "    'reg_lambda': [0, 0.1, 1, 5],\n",
    "}\n",
    "\n",
    "# 7. RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring='neg_mean_absolute_error',  # using MAE for minimization\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Starting hyperparameter tuning...\")\n",
    "start_time = time.time()\n",
    "search.fit(X_train_sel, y_train)\n",
    "tuning_time = time.time() - start_time\n",
    "print(f\"‚úÖ Tuning done in {tuning_time:.2f} seconds\")\n",
    "\n",
    "best_params = search.best_params_\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# 8. Train best model on train+val\n",
    "X_full = np.vstack([X_train_sel, X_val_sel])\n",
    "y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "best_model = xgb.XGBRegressor(\n",
    "    **best_params,\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    eval_metric='rmse',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Training final model on full data...\")\n",
    "start_train = time.time()\n",
    "best_model.fit(X_full, y_full)\n",
    "train_time = time.time() - start_train\n",
    "print(f\"‚úÖ Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "# 9. Evaluation on validation set (for reference)\n",
    "val_preds = best_model.predict(X_val_sel)\n",
    "rel_mae = np.mean(np.abs((val_preds - y_val) / y_val))\n",
    "rmse = mean_squared_error(y_val, val_preds)\n",
    "r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "print(\"\\nüìä Evaluation on validation set:\")\n",
    "print(f\"Relative MAE: {rel_mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "# 10. Save model and scaler\n",
    "model_path = \"regression_xgboost_model.json\"\n",
    "best_model.save_model(model_path)\n",
    "scaler_path = \"scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "model_size_MB = os.path.getsize(model_path) / (1024**2)\n",
    "scaler_size_MB = os.path.getsize(scaler_path) / (1024**2)\n",
    "print(f\"üíæ Model size: {model_size_MB:.2f} MB\")\n",
    "print(f\"üíæ Scaler size: {scaler_size_MB:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51553fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions and variable list saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load test data\n",
    "test_df = pd.read_csv('./data/dataset_1/AppML_InitialProject_test_regression.csv')\n",
    "\n",
    "# 2. Select the top 12 features used during training\n",
    "X_test_sel = test_df[top_12_features]\n",
    "\n",
    "# 3. Scale using the same scaler fitted on training data\n",
    "X_test_sel_scaled = scaler.transform(X_test_sel)\n",
    "\n",
    "# 4. Predict with XGBoost model\n",
    "predictions = best_model.predict(X_test_sel_scaled)\n",
    "\n",
    "# 5. Save predictions CSV with index and prediction, no header\n",
    "output_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "output_df.to_csv('Regression_HusainManasawala_XGBoost.csv', index=False, header=False)\n",
    "\n",
    "# 6. Save top 12 features in comma + newline style\n",
    "with open('Regression_HusainManasawala_XGBoost_VariableList.csv', 'w') as f:\n",
    "    for feature in top_12_features:\n",
    "        f.write(f\"{feature},\\n\")\n",
    "\n",
    "print(\"‚úÖ Predictions and variable list saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed28acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total split nodes: 10403\n",
      "Total leaf nodes: 10603\n",
      "Total estimated parameters (splits + leaves): 21006\n"
     ]
    }
   ],
   "source": [
    "def count_nodes_xgb(tree):\n",
    "    \"\"\"Recursively count split nodes and leaf nodes in an XGBoost tree.\"\"\"\n",
    "    if 'children' not in tree:\n",
    "        # Leaf node\n",
    "        return (0, 1)  # (split_nodes, leaf_nodes)\n",
    "    else:\n",
    "        split_nodes = 1  # current split node\n",
    "        leaf_nodes = 0\n",
    "        for child in tree['children']:\n",
    "            s, l = count_nodes_xgb(child)\n",
    "            split_nodes += s\n",
    "            leaf_nodes += l\n",
    "        return (split_nodes, leaf_nodes)\n",
    "\n",
    "import json\n",
    "\n",
    "model_dump = xgb_reg.get_booster().get_dump(dump_format='json')\n",
    "\n",
    "total_splits = 0\n",
    "total_leaves = 0\n",
    "for tree_json in model_dump:\n",
    "    tree_dict = json.loads(tree_json)\n",
    "    splits, leaves = count_nodes_xgb(tree_dict)\n",
    "    total_splits += splits\n",
    "    total_leaves += leaves\n",
    "\n",
    "total_params = total_splits + total_leaves\n",
    "print(f\"Total split nodes: {total_splits}\")\n",
    "print(f\"Total leaf nodes: {total_leaves}\")\n",
    "print(f\"Total estimated parameters (splits + leaves): {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6635ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 features: ['pX_maxEcell_energy', 'pX_ecore', 'p_etcone20', 'pX_E_Lr2_MedG', 'p_ptcone40', 'pX_e233', 'pX_etcone20', 'pX_topoetcone20', 'p_pt_track', 'pX_E3x5_Lr0', 'p_ptcone30', 'p_sigmad0']\n",
      "Using device: cuda\n",
      "‚è≥ Starting Optuna tuning...\n",
      "‚úÖ Best hyperparameters: {'hidden_dim': 224, 'lr': 0.00045677333120969536}\n",
      "‚è±Ô∏è Tuning time: 338.20 seconds\n",
      "‚úÖ Final training time: 69.52 seconds\n",
      "\n",
      "üìä Final Evaluation:\n",
      "Relative MAE: 0.3527\n",
      "RMSE: 18849.1935\n",
      "R¬≤: 0.8861\n",
      "üíæ Saved model size: 0.40 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import time\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- 1. Load and filter data ---\n",
    "df = pd.read_csv('./data/dataset_1/AppML_InitialProject_train.csv')\n",
    "df = df[df[\"p_Truth_isElectron\"] == 1]\n",
    "\n",
    "target = \"p_Truth_Energy\"\n",
    "X = df.drop(columns=[\"p_Truth_isElectron\", \"p_Truth_Energy\"])\n",
    "y = df[target]\n",
    "\n",
    "# --- 2. Train/Val split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 3. Feature selection: Top 12 via RF importance ---\n",
    "rf_reg = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "feat_importances = pd.Series(rf_reg.feature_importances_, index=X_train.columns)\n",
    "top_12_features = feat_importances.nlargest(12).index.tolist()\n",
    "print(\"Top 12 features:\", top_12_features)\n",
    "\n",
    "X_train_sel = X_train[top_12_features]\n",
    "X_val_sel = X_val[top_12_features]\n",
    "\n",
    "# --- 4. Scaling features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_sel = scaler.fit_transform(X_train_sel)\n",
    "X_val_sel = scaler.transform(X_val_sel)\n",
    "\n",
    "# --- 4b. Scale target ---\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "# --- 5. Convert to torch tensors (float32) ---\n",
    "X_train_t = torch.tensor(X_train_sel, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_val_t = torch.tensor(X_val_sel, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "# --- 6. Setup device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 7. Define neural network ---\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- 8. Training function ---\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss = criterion(preds, yb)\n",
    "                val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# --- 9. Evaluation function (with inverse target scaling) ---\n",
    "def evaluate(model, X, y_scaled, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_scaled = model(X.to(device)).cpu().numpy().flatten()\n",
    "    preds = target_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "    y_true = target_scaler.inverse_transform(y_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
    "    rel_mae = np.mean(np.abs((preds - y_true) / y_true))\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    r2 = r2_score(y_true, preds)\n",
    "    return rel_mae, rmse, r2, preds\n",
    "\n",
    "# --- 10. Prepare DataLoaders ---\n",
    "def get_dataloader(X, y, batch_size=64):\n",
    "    ds = TensorDataset(X, y)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = get_dataloader(X_train_t, y_train_t)\n",
    "val_loader = get_dataloader(X_val_t, y_val_t, batch_size=256)\n",
    "\n",
    "# --- 11. Optuna objective ---\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 300)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    epochs = 50  # fixed for tuning\n",
    "\n",
    "    model = FeedForwardNN(input_dim=12, hidden_dim=hidden_dim).to(device)\n",
    "    trained_model = train_model(model, train_loader, val_loader, epochs, lr, device)\n",
    "\n",
    "    rel_mae, rmse, r2, _ = evaluate(trained_model, X_val_t, y_val_t, device)\n",
    "    return rel_mae\n",
    "\n",
    "# --- 12. Run Optuna tuning ---\n",
    "print(\"‚è≥ Starting Optuna tuning...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10, timeout=900)\n",
    "optuna_time = time.time() - start_time\n",
    "print(f\"‚úÖ Best hyperparameters: {study.best_params}\")\n",
    "print(f\"‚è±Ô∏è Tuning time: {optuna_time:.2f} seconds\")\n",
    "\n",
    "# --- 13. Train final model ---\n",
    "best_params = study.best_params\n",
    "best_hidden_dim = best_params[\"hidden_dim\"]\n",
    "best_lr = best_params[\"lr\"]\n",
    "epochs_final = 100  # longer training for final\n",
    "\n",
    "model = FeedForwardNN(input_dim=12, hidden_dim=best_hidden_dim).to(device)\n",
    "start_train_time = time.time()\n",
    "model = train_model(model, train_loader, val_loader, epochs_final, best_lr, device)\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\"‚úÖ Final training time: {train_time:.2f} seconds\")\n",
    "\n",
    "# --- 14. Evaluate final model ---\n",
    "rel_mae, rmse, r2, preds = evaluate(model, X_val_t, y_val_t, device)\n",
    "print(\"\\nüìä Final Evaluation:\")\n",
    "print(f\"Relative MAE: {rel_mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "# --- 15. Save model ---\n",
    "model_path = \"pytorch_nn_regressor.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'target_scaler': target_scaler,  # Save target scaler for inference!\n",
    "    'features': top_12_features,\n",
    "    'params': best_params\n",
    "}, model_path)\n",
    "\n",
    "model_size_MB = os.path.getsize(model_path) / (1024**2)\n",
    "print(f\"üíæ Saved model size: {model_size_MB:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b010088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions saved to pytorch_nn_predictions.csv\n",
      "‚úÖ Variable list saved to Regression_HusainManasawala_PyTorchNN_VariableList.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Load test data ---\n",
    "df_test = pd.read_csv('./data/dataset_1/AppML_InitialProject_test_regression.csv')\n",
    "X_test = df_test[top_12_features]\n",
    "\n",
    "# --- Scale test features ---\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Convert to torch tensor ---\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# --- Predict with model ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_scaled = model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "# --- Inverse scale predictions ---\n",
    "preds = target_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# --- Save predictions ---\n",
    "output_df = pd.DataFrame({\n",
    "    \"index\": df_test.index,          # original index from test data\n",
    "    \"p_Truth_Energy_Pred\": preds\n",
    "})\n",
    "\n",
    "output_df.to_csv(\"Regression_HusainManasawala_PyTorchNN.csv\", index=False, header=False)\n",
    "print(\"‚úÖ Predictions saved to pytorch_nn_predictions.csv\")\n",
    "\n",
    "# --- Save variable list with commas and line breaks ---\n",
    "with open('Regression_HusainManasawala_PyTorchNN_VariableList.csv', 'w') as f:\n",
    "    for feature in top_12_features:\n",
    "        f.write(f\"{feature},\\n\")\n",
    "\n",
    "print(\"‚úÖ Variable list saved to Regression_HusainManasawala_PyTorchNN_VariableList.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions saved to pytorch_nn_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a1699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_copenhagen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
